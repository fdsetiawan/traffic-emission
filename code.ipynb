{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load emission data \n",
    "import pandas as pd\n",
    "emission_data = pd.read_excel('emission_data.xlsx')\n",
    "\n",
    "emission_data['Date_and_Time'] = pd.to_datetime(emission_data['Date_and_Time'])\n",
    "emission_data = emission_data.drop(columns=['Station_Name', 'Latitude', 'Longitude', 'HC', 'Temperature', 'Humidity', 'Rainfall', 'Wind_Speed',\n",
    "       'Wind_Direction', 'Global_Radiation']).dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Traffic Data\n",
    "# Load traffic data\n",
    "traffic_data = pd.read_excel('traffic_data.xlsx')\n",
    "traffic_data['Date_and_Time'] = pd.to_datetime(traffic_data['Date_and_Time'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge traffic emission data\n",
    "traffic_emission_data = pd.merge(traffic_data, emission_data, on='Date_and_Time', how='left')\n",
    "\n",
    "traffic_emission_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_emission_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data distribution (box plot)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Define the variables\n",
    "# variables_plot = ['Total_Motorcycle', \n",
    "#                   'Total_Car', \n",
    "#                   'Total_BusTruck', \n",
    "#                   'Total_Total', \n",
    "#                   'Total_Velocity', \n",
    "#                   'Total_PCE', \n",
    "#                   'Total_VCRatio', \n",
    "#                   'PM10']\n",
    "\n",
    "variables_plot = traffic_emission_data.columns[1:]\n",
    "\n",
    "\n",
    "\n",
    "# Plot box plots\n",
    "plt.figure(figsize=(10, 40))\n",
    "for i, var in enumerate(variables_plot):\n",
    "    plt.subplot(9, 3, i+1)\n",
    "    sns.boxplot(data=traffic_emission_data, y=var)\n",
    "    plt.title(f'Box Plot of {var}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to remove outliers, more first\n",
    "import pandas as pd\n",
    "\n",
    "def remove_outliers(df, variables):\n",
    "    \"\"\"\n",
    "    Removes outliers from specified variables in the DataFrame using the IQR method.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): The DataFrame to clean.\n",
    "        variables (list): A list of variable names from which to remove outliers.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: A DataFrame with outliers removed from the specified variables.\n",
    "    \"\"\"\n",
    "    clean_df = df.copy()\n",
    "    for var in variables:\n",
    "        # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "        Q1 = clean_df[var].quantile(0.25)\n",
    "        Q3 = clean_df[var].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        # Define bounds for outliers\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Filter out outliers\n",
    "        clean_df = clean_df[(clean_df[var] >= lower_bound) & (clean_df[var] <= upper_bound)]\n",
    "    \n",
    "    return clean_df\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def count_and_sort_outliers(data, variables):\n",
    "    # Normalizing the data\n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = pd.DataFrame(scaler.fit_transform(data[variables]), columns=variables)\n",
    "    \n",
    "    outlier_counts = {}\n",
    "    \n",
    "    # Counting outliers for each variable\n",
    "    for variable in variables:\n",
    "        Q1 = data_scaled[variable].quantile(0.25)\n",
    "        Q3 = data_scaled[variable].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = data_scaled[(data_scaled[variable] < lower_bound) | (data_scaled[variable] > upper_bound)]\n",
    "        outlier_counts[variable] = len(outliers)\n",
    "    \n",
    "    # Sorting variables by number of outliers\n",
    "    sorted_variables = sorted(outlier_counts, key=outlier_counts.get, reverse=True)\n",
    "    \n",
    "    return sorted_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define traffic variables\n",
    "traffic_vars = ['Total_Motorcycle', 'Total_Car', 'Total_BusTruck', 'Total_Total', 'Total_PCE', 'Total_Velocity', 'Total_VCRatio']\n",
    "\n",
    "# Define emission variables\n",
    "emission_vars = ['PM10', 'PM2_5', 'SO2', 'CO', 'O3', 'NO2']\n",
    "# emission_vars = ['PM10']\n",
    "\n",
    "# Sort and Clean outliers\n",
    "sorted_vars = count_and_sort_outliers(traffic_emission_data, traffic_vars+emission_vars)\n",
    "data_clean = remove_outliers(traffic_emission_data, sorted_vars)\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_clean[['Date_and_Time',\n",
    "                   'Total_Motorcycle',\n",
    "                   'Total_Car',\n",
    "                   'Total_BusTruck',\n",
    "                   'Total_Total',\n",
    "                   'Total_PCE',\n",
    "                   'Total_Velocity',\n",
    "                   'Total_VCRatio',\n",
    "                   'PM10']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Define the variables\n",
    "variables_plot = ['Total_Motorcycle', \n",
    "                  'Total_Car', \n",
    "                  'Total_BusTruck', \n",
    "                  'Total_Total', \n",
    "                  'Total_Velocity', \n",
    "                  'Total_PCE', \n",
    "                  'Total_VCRatio', \n",
    "                  'PM10']\n",
    "\n",
    "\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(18, 12))\n",
    "for i, var in enumerate(variables_plot):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    sns.histplot(data=data, x=var, kde=True)\n",
    "    plt.title(f'Histogram of {var}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot box plots\n",
    "plt.figure(figsize=(18, 12))\n",
    "for i, var in enumerate(variables_plot):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    sns.boxplot(data=data, y=var)\n",
    "    plt.title(f'Box Plot of {var}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting features from 'Date_and_Time'\n",
    "data['Hour'] = data['Date_and_Time'].dt.hour\n",
    "data['Day'] = data['Date_and_Time'].dt.day\n",
    "data['Month'] = data['Date_and_Time'].dt.month\n",
    "data['Weekday'] = data['Date_and_Time'].dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Group by Hour to get the average values for each hour\n",
    "hourly_data = data.groupby('Hour').mean().reset_index()\n",
    "\n",
    "# Define traffic and PM emission variables\n",
    "traffic_vars = [\n",
    "    'Total_Motorcycle', \n",
    "    'Total_Car', \n",
    "    'Total_BusTruck',\n",
    "    'Total_Total', \n",
    "    'Total_Velocity', \n",
    "    'Total_PCE', \n",
    "    'Total_VCRatio'\t\n",
    "    ]\n",
    "pm_vars = ['PM10']\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "hourly_data[traffic_vars + pm_vars] = scaler.fit_transform(hourly_data[traffic_vars + pm_vars])\n",
    "\n",
    "# Plotting hourly trends for traffic and PM emission variables\n",
    "plt.figure(figsize=(12, 6))\n",
    "for var in traffic_vars:\n",
    "    sns.lineplot(data=hourly_data, x='Hour', y=var, label=var, ci=None)\n",
    "for var in pm_vars:\n",
    "    sns.lineplot(data=hourly_data, x='Hour', y=var, label=var, linestyle='--', ci=None)\n",
    "plt.title('Normalized Hourly Trends of Traffic and PM Emission Levels')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Normalized Value')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.xticks(hourly_data['Hour'].unique())  # Only show hours that have data\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix for the selected variables\n",
    "correlation_matrix = data[traffic_vars + pm_vars].corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation between Traffic Variables and PM10 Emission Levels')\n",
    "plt.xlabel('Variables')\n",
    "plt.ylabel('Variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load dataset\n",
    "selected_column = ['Date_and_Time',\t'Total_Motorcycle',\t'Total_Car',\t'Total_BusTruck',\t'Total_Total',\t'Total_PCE',\t\n",
    "                 'Total_Velocity',\t'Total_VCRatio',\t'PM10'\t]\n",
    "# selected_column = ['Date_and_Time',\t'Total_Motorcycle',\t'Total_Car',\t'Total_BusTruck',\t'PM10'\t]\n",
    "data_model = data[selected_column].copy()\n",
    "\n",
    "\n",
    "\n",
    "data_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparation with k-Fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_model.drop(columns=['Date_and_Time', 'PM10'])\n",
    "y = data_model['PM10']\n",
    "\n",
    "# Define models with pipelines that include scaling\n",
    "models = {\n",
    "    'SVM': Pipeline([('scaler', StandardScaler()), ('svm', SVR())]),\n",
    "    'Random Forest': RandomForestRegressor(),  # No scaling needed for Random Forest\n",
    "    'MLP': Pipeline([('scaler', StandardScaler()), ('mlp', MLPRegressor(max_iter=1000))])\n",
    "}\n",
    "\n",
    "# Set up k-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model_r2(model, X, y, kf):\n",
    "    scores = cross_val_score(model, X, y, scoring='r2', cv=kf)\n",
    "    return scores  # Return R² scores\n",
    "\n",
    "# Dictionary to hold the results\n",
    "results = {}\n",
    "\n",
    "# Evaluate each model using k-fold cross-validation\n",
    "for model_name, model in models.items():\n",
    "    r2_scores = evaluate_model_r2(model, X, y, kf)\n",
    "    results[model_name] = r2_scores\n",
    "    print(f'{model_name} R²: {r2_scores.mean():.4f} ± {r2_scores.std():.4f}')\n",
    "\n",
    "# Convert results to a DataFrame for better readability\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nModel Selection Results (R²):\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting features from 'Date_and_Time'\n",
    "data_model['Hour'] = data['Date_and_Time'].dt.hour\n",
    "data_model['Day'] = data['Date_and_Time'].dt.day\n",
    "data_model['Month'] = data['Date_and_Time'].dt.month\n",
    "data_model['Weekday'] = data['Date_and_Time'].dt.weekday\n",
    "\n",
    "X = data_model.drop(columns=['Date_and_Time', 'PM10'])\n",
    "y = data_model['PM10']\n",
    "\n",
    "# Define models with pipelines that include scaling\n",
    "models = {\n",
    "    'SVM': Pipeline([('scaler', StandardScaler()), ('svm', SVR())]),\n",
    "    'Random Forest': RandomForestRegressor(),  # No scaling needed for Random Forest\n",
    "    'MLP': Pipeline([('scaler', StandardScaler()), ('mlp', MLPRegressor(max_iter=1000))])\n",
    "}\n",
    "\n",
    "# Set up k-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model_r2(model, X, y, kf):\n",
    "    scores = cross_val_score(model, X, y, scoring='r2', cv=kf)\n",
    "    return scores  # Return R² scores\n",
    "\n",
    "# Dictionary to hold the results\n",
    "results = {}\n",
    "\n",
    "# Evaluate each model using k-fold cross-validation\n",
    "for model_name, model in models.items():\n",
    "    r2_scores = evaluate_model_r2(model, X, y, kf)\n",
    "    results[model_name] = r2_scores\n",
    "    print(f'{model_name} R²: {r2_scores.mean():.4f} ± {r2_scores.std():.4f}')\n",
    "\n",
    "# Convert results to a DataFrame for better readability\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nModel Selection Results (R²):\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected RF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial RF Model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data_model.drop(columns=['Date_and_Time', 'PM10'])\n",
    "y = data_model['PM10']\n",
    "\n",
    "# Assume X and y are already defined as your features and target variable\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define Random Forest model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "features = X.columns  # assuming X is a pandas DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the feature importances\n",
    "print(importance_df)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances in Random Forest')\n",
    "plt.gca().invert_yaxis()  # To display the highest importance at the top\n",
    "plt.show()\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = rf.predict(X_train)\n",
    "y_pred_test = rf.predict(X_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "n = X_test.shape[0]\n",
    "p = X_test.shape[1]\n",
    "adjusted_r2 = 1 - (1 - r2_test) * (n - 1) / (n - p - 1)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred_test)\n",
    "mse = mean_squared_error(y_test, y_pred_test)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Mean Forecast Error (MFE) and Mean Relative Error (MRE)\n",
    "mfe = np.mean(y_test - y_pred_test)\n",
    "# mre = np.mean(np.abs((y_test - y_pred_test) / y_test))\n",
    "\n",
    "# Filter out zero values in y_test to avoid division by zero\n",
    "non_zero_indices = y_test != 0\n",
    "y_test_non_zero = y_test[non_zero_indices]\n",
    "y_pred_test_non_zero = y_pred_test[non_zero_indices]\n",
    "mre = np.mean(np.abs((y_test_non_zero - y_pred_test_non_zero) / y_test_non_zero))\n",
    "\n",
    "\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f'R² (train): {r2_train:.4f}')\n",
    "print(f'R² (test): {r2_test:.4f}')\n",
    "print(f'Adjusted R² (test): {adjusted_r2:.4f}')\n",
    "print(f'MAE: {mae:.4f}')\n",
    "print(f'MSE: {mse:.4f}')\n",
    "print(f'RMSE: {rmse:.4f}')\n",
    "print(f'MFE: {mfe:.4f}')\n",
    "print(f'MRE: {mre:.4f}')\n",
    "\n",
    "# Plot actual vs predicted\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(y_test, y_pred_test, edgecolors=(0, 0, 0))\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--', lw=3)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs Predicted')\n",
    "plt.show()\n",
    "\n",
    "# Plot residuals vs predicted\n",
    "residuals = y_test - y_pred_test\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(y_pred_test, residuals, edgecolors=(0, 0, 0))\n",
    "plt.hlines(y=0, xmin=min(y_pred_test), xmax=max(y_pred_test), colors='r', linestyles='dashed')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a threshold for feature selection\n",
    "threshold = 0.03  # This threshold can be adjusted based on your specific needs\n",
    "\n",
    "# Select features above the threshold\n",
    "selected_features = importance_df[importance_df['Importance'] > threshold]['Feature']\n",
    "X_train_reduced = X_train[selected_features]\n",
    "X_test_reduced = X_test[selected_features]\n",
    "\n",
    "\n",
    "# Define Random Forest model\n",
    "rf_reduced = RandomForestRegressor(random_state=42)\n",
    "rf_reduced.fit(X_train_reduced, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances_reduced = rf_reduced.feature_importances_\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "importance_df_reduced = pd.DataFrame({\n",
    "    'Feature': selected_features,\n",
    "    'Importance': feature_importances_reduced\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "importance_df_reduced = importance_df_reduced.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the feature importances\n",
    "print(importance_df_reduced)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df_reduced['Feature'], importance_df_reduced['Importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances in Random Forest (Reduced Features)')\n",
    "plt.gca().invert_yaxis()  # To display the highest importance at the top\n",
    "plt.show()\n",
    "\n",
    "# Predictions for reduced model\n",
    "y_pred_train_reduced = rf_reduced.predict(X_train_reduced)\n",
    "y_pred_test_reduced = rf_reduced.predict(X_test_reduced)\n",
    "\n",
    "# Evaluation metrics for reduced model\n",
    "r2_train_reduced = r2_score(y_train, y_pred_train_reduced)\n",
    "r2_test_reduced = r2_score(y_test, y_pred_test_reduced)\n",
    "\n",
    "n = X_test_reduced.shape[0]\n",
    "p = X_test_reduced.shape[1]\n",
    "adjusted_r2_reduced = 1 - (1 - r2_test_reduced) * (n - 1) / (n - p - 1)\n",
    "\n",
    "mae_reduced = mean_absolute_error(y_test, y_pred_test_reduced)\n",
    "mse_reduced = mean_squared_error(y_test, y_pred_test_reduced)\n",
    "rmse_reduced = np.sqrt(mse_reduced)\n",
    "\n",
    "# Mean Forecast Error (MFE) and Mean Relative Error (MRE) for reduced model\n",
    "mfe_reduced = np.mean(y_test - y_pred_test_reduced)\n",
    "# mre_reduced = np.mean(np.abs((y_test - y_pred_test_reduced) / y_test))\n",
    "\n",
    "# Filter out zero values in y_test to avoid division by zero\n",
    "non_zero_indices = y_test != 0\n",
    "y_test_non_zero = y_test[non_zero_indices]\n",
    "y_pred_test_reduced_non_zero = y_pred_test_reduced[non_zero_indices]\n",
    "mre_reduced = np.mean(np.abs((y_test_non_zero - y_pred_test_reduced_non_zero) / y_test_non_zero))\n",
    "\n",
    "\n",
    "\n",
    "# Print evaluation metrics for reduced model\n",
    "print(f'R² (train): {r2_train_reduced:.4f}')\n",
    "print(f'R² (test): {r2_test_reduced:.4f}')\n",
    "print(f'Adjusted R² (test): {adjusted_r2_reduced:.4f}')\n",
    "print(f'MAE: {mae_reduced:.4f}')\n",
    "print(f'MSE: {mse_reduced:.4f}')\n",
    "print(f'RMSE: {rmse_reduced:.4f}')\n",
    "print(f'MFE: {mfe_reduced:.4f}')\n",
    "print(f'MRE: {mre_reduced:.4f}')\n",
    "\n",
    "# Plot actual vs predicted for reduced model\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(y_test, y_pred_test_reduced, edgecolors=(0, 0, 0))\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--', lw=3)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs Predicted (Reduced Features)')\n",
    "plt.show()\n",
    "\n",
    "# Plot residuals vs predicted for reduced model\n",
    "residuals_reduced = y_test - y_pred_test_reduced\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(y_pred_test_reduced, residuals_reduced, edgecolors=(0, 0, 0))\n",
    "plt.hlines(y=0, xmin=min(y_pred_test_reduced), xmax=max(y_pred_test_reduced), colors='r', linestyles='dashed')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Predicted (Reduced Features)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameter space for GridSearchCV\n",
    "param_grid1 = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search1 = GridSearchCV(estimator=rf, param_grid=param_grid1,\n",
    "                           cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search1.fit(X_train_reduced, y_train)\n",
    "\n",
    "# Best parameters from GridSearchCV\n",
    "best_params_grid1 = grid_search1.best_params_\n",
    "best_model_grid1 = grid_search1.best_estimator_\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters from GridSearchCV: \", best_params_grid1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_score_grid1 = best_model_grid1.score(X_test_reduced, y_test)\n",
    "print(\"Test set score with best parameters from GridSearchCV: \", test_score_grid1)\n",
    "\n",
    "# Predictions for reduced model\n",
    "y_pred_train_grid1 = best_model_grid1.predict(X_train_reduced)\n",
    "y_pred_test_grid1 = best_model_grid1.predict(X_test_reduced)\n",
    "\n",
    "# Evaluation metrics for GridSearchCV model\n",
    "r2_train_grid1 = r2_score(y_train, y_pred_train_grid1)\n",
    "r2_test_grid1 = r2_score(y_test, y_pred_test_grid1)\n",
    "\n",
    "print(f'R² (train): {r2_train_grid1:.4f}')\n",
    "print(f'R² (test): {r2_test_grid1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter space for second GridSearchCV\n",
    "param_grid2 = {\n",
    "    'bootstrap': [True, False],\n",
    "    'criterion': ['squared_error', 'absolute_error'],\n",
    "    'oob_score': [True, False],\n",
    "    'n_estimators': [best_params_grid1['n_estimators']],\n",
    "    'max_depth': [best_params_grid1['max_depth']],\n",
    "    'min_samples_split': [best_params_grid1['min_samples_split']],\n",
    "    'min_samples_leaf': [best_params_grid1['min_samples_leaf']],\n",
    "    'max_features': [best_params_grid1['max_features']]\n",
    "}\n",
    "\n",
    "# Define Random Forest model with best parameters from first GridSearchCV\n",
    "rf_tuned = RandomForestRegressor(\n",
    "    random_state=42,\n",
    "    n_estimators=best_params_grid1['n_estimators'],\n",
    "    max_depth=best_params_grid1['max_depth'],\n",
    "    min_samples_split=best_params_grid1['min_samples_split'],\n",
    "    min_samples_leaf=best_params_grid1['min_samples_leaf'],\n",
    "    max_features=best_params_grid1['max_features']\n",
    ")\n",
    "\n",
    "# Perform GridSearchCV again with additional parameters\n",
    "grid_search2 = GridSearchCV(estimator=rf_tuned, param_grid=param_grid2,\n",
    "                            cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search2.fit(X_train_reduced, y_train)\n",
    "\n",
    "# Best parameters from second GridSearchCV\n",
    "best_params_grid2 = grid_search2.best_params_\n",
    "best_model_grid2 = grid_search2.best_estimator_\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters from second GridSearchCV: \", best_params_grid2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_score_grid2 = best_model_grid2.score(X_test_reduced, y_test)\n",
    "print(\"Test set score with best parameters from second GridSearchCV: \", test_score_grid2)\n",
    "\n",
    "# Predictions for reduced model\n",
    "y_pred_train_grid2 = best_model_grid2.predict(X_train_reduced)\n",
    "y_pred_test_grid2 = best_model_grid2.predict(X_test_reduced)\n",
    "\n",
    "# Evaluation metrics for second GridSearchCV model\n",
    "r2_train_grid2 = r2_score(y_train, y_pred_train_grid2)\n",
    "r2_test_grid2 = r2_score(y_test, y_pred_test_grid2)\n",
    "\n",
    "print(f'R² (train): {r2_train_grid2:.4f}')\n",
    "print(f'R² (test): {r2_test_grid2:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Random Forest model with best parameters from the second GridSearchCV\n",
    "rf_tuned = RandomForestRegressor(\n",
    "    random_state=42,\n",
    "    n_estimators=best_params_grid2['n_estimators'],\n",
    "    max_depth=best_params_grid2['max_depth'],\n",
    "    min_samples_split=best_params_grid2['min_samples_split'],\n",
    "    min_samples_leaf=best_params_grid2['min_samples_leaf'],\n",
    "    max_features=best_params_grid2['max_features'],\n",
    "    bootstrap=best_params_grid2['bootstrap'],\n",
    "    criterion=best_params_grid2['criterion'],\n",
    "    oob_score=best_params_grid2['oob_score']\n",
    ")\n",
    "\n",
    "# Train the model with reduced features and best parameters\n",
    "rf_tuned.fit(X_train_reduced, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances_tuned = rf_tuned.feature_importances_\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "importance_df_tuned = pd.DataFrame({\n",
    "    'Feature': selected_features,\n",
    "    'Importance': feature_importances_tuned\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "importance_df_tuned = importance_df_tuned.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the feature importances\n",
    "print(importance_df_tuned)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df_tuned['Feature'], importance_df_tuned['Importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances in Random Forest (Tuned Hyperparameter)')\n",
    "plt.gca().invert_yaxis()  # To display the highest importance at the top\n",
    "plt.show()\n",
    "\n",
    "# Predictions for reduced model\n",
    "y_pred_train_tuned = rf_tuned.predict(X_train_reduced)\n",
    "y_pred_test_tuned = rf_tuned.predict(X_test_reduced)\n",
    "\n",
    "# Evaluation metrics for reduced model\n",
    "r2_train_tuned = r2_score(y_train, y_pred_train_tuned)\n",
    "r2_test_tuned = r2_score(y_test, y_pred_test_tuned)\n",
    "\n",
    "n = X_test_reduced.shape[0]\n",
    "p = X_test_reduced.shape[1]\n",
    "adjusted_r2_tuned = 1 - (1 - r2_test_tuned) * (n - 1) / (n - p - 1)\n",
    "\n",
    "mae_tuned = mean_absolute_error(y_test, y_pred_test_tuned)\n",
    "mse_tuned = mean_squared_error(y_test, y_pred_test_tuned)\n",
    "rmse_tuned = np.sqrt(mse_tuned)\n",
    "\n",
    "# Mean Forecast Error (MFE) and Mean Relative Error (MRE) for tuned model\n",
    "mfe_tuned = np.mean(y_test - y_pred_test_tuned)\n",
    "\n",
    "# Filter out zero values in y_test to avoid division by zero\n",
    "non_zero_indices = y_test != 0\n",
    "y_test_non_zero = y_test[non_zero_indices]\n",
    "y_pred_test_tuned_non_zero = y_pred_test_tuned[non_zero_indices]\n",
    "mre_tuned = np.mean(np.abs((y_test_non_zero - y_pred_test_tuned_non_zero) / y_test_non_zero))\n",
    "\n",
    "# Print evaluation metrics for reduced model\n",
    "print(f'R² (train): {r2_train_tuned:.4f}')\n",
    "print(f'R² (test): {r2_test_tuned:.4f}')\n",
    "print(f'Adjusted R² (test): {adjusted_r2_tuned:.4f}')\n",
    "print(f'MAE: {mae_tuned:.4f}')\n",
    "print(f'MSE: {mse_tuned:.4f}')\n",
    "print(f'RMSE: {rmse_tuned:.4f}')\n",
    "print(f'MFE: {mfe_tuned:.4f}')\n",
    "print(f'MRE: {mre_tuned:.4f}')\n",
    "\n",
    "# Plot actual vs predicted for reduced model\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(y_test, y_pred_test_tuned, edgecolors=(0, 0, 0))\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--', lw=3)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs Predicted (Tuned Hyperparameter)')\n",
    "plt.show()\n",
    "\n",
    "# Plot residuals vs predicted for tuned model\n",
    "residuals_tuned = y_test - y_pred_test_tuned\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(y_pred_test_tuned, residuals_tuned, edgecolors=(0, 0, 0))\n",
    "plt.hlines(y=0, xmin=min(y_pred_test_tuned), xmax=max(y_pred_test_tuned), colors='r', linestyles='dashed')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Predicted (Tuned Hyperparameter)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-Series Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_LSTM = data[['Date_and_Time','PM10']]\n",
    "data_LSTM.set_index('Date_and_Time', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Data preparation function for LSTM\n",
    "def prepare_data(data, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(data)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = data[i:end_ix], data[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Parameters\n",
    "n_steps = 24  # Number of time steps for each input\n",
    "n_features = 1  # PM10 is a single feature\n",
    "\n",
    "\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_scaled = scaler.fit_transform(data_LSTM.values.reshape(-1, 1))\n",
    "\n",
    "# Prepare the data\n",
    "X, y = prepare_data(data_scaled, n_steps)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "train_size = int(len(X) * 0.8)  # 80% for training\n",
    "X_train, X_test = X[:train_size], X[train_size:]  # Ambil data awal untuk training\n",
    "y_train, y_test = y[:train_size], y[train_size:]  # Sisakan data akhir untuk testing\n",
    "\n",
    "\n",
    "# Reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], n_features))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], n_features))\n",
    "\n",
    "# Create the LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "# Add LSTM layers with dropout\n",
    "for _ in range(6):  # 6 LSTM layers\n",
    "    model.add(LSTM(units=30, activation='relu', return_sequences=True))  # 30 units per LSTM layer\n",
    "    model.add(Dropout(0.2))  # Dropout rate of 0.2\n",
    "\n",
    "# Adding a final LSTM layer without return_sequences\n",
    "model.add(LSTM(units=30, activation='relu'))  \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1))  # Assuming it's a regression problem with one output\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Fit the model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "# history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping], shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Rescale predictions back to the original scale\n",
    "predictions_rescaled = scaler.inverse_transform(predictions)\n",
    "y_test_rescaled = scaler.inverse_transform(y_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "rmse = mean_squared_error(y_test_rescaled, predictions_rescaled, squared=False)\n",
    "mae = mean_absolute_error(y_test_rescaled, predictions_rescaled)\n",
    "r2 = r2_score(y_test_rescaled, predictions_rescaled)\n",
    "\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'R2: {r2}')\n",
    "\n",
    "# Plot the loss\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(19, 6))\n",
    "plt.plot(y_test_rescaled, label='Actual', color='blue', marker='o')\n",
    "plt.plot(predictions_rescaled, label='Predicted', color='red', linestyle='--', marker='x')\n",
    "plt.title('PM10 Actual vs Predicted')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('PM10 Concentration')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
